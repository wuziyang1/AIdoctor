{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1.构建NER模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcb3e70f791c3b55"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": true
   },
   "id": "initial_id",
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1定义BiLSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c413f66c355a235f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, label_num):\n",
    "        super(BiLSTM, self).__init__()\n",
    "\n",
    "        # embeding字嵌入\n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=256,device=device)\n",
    "\n",
    "        # bilstm，没有BiLSTM对象，只有LSTM，bidirectional\n",
    "        self.blstm = nn.LSTM(input_size=256,hidden_size=512,bidirectional=True,num_layers=1)\n",
    "\n",
    "        # 线性层, 最终输出是发射概率矩阵\n",
    "        self.linear = nn.Linear(in_features=1024, out_features=label_num)#正向512反向512整合后1024\n",
    "        # label_num：{\"O\": 0, \"B-dis\": 1, \"I-dis\": 2, \"B-sym\": 3, \"I-sym\": 4}共5个\n",
    "        #这里输出的就是观测概率矩阵：就是把一句话转BMSE这种用单词表示的形式\n",
    "\n",
    "    def forward(self, inputs, length):\n",
    "        # 嵌入层，得到向量\n",
    "        outputs_embed = self.embed(inputs)#[batch_size,seq_len,num_features]\n",
    "\n",
    "        # 训练的时候打了padding。由于padding的时候填充了很多0，在这里送入模型的时候把padding去掉\n",
    "        outputs_packd = pack_padded_sequence(outputs_embed, length.cpu())\n",
    "\n",
    "        # 把压缩后的结果输入到lstm中\n",
    "        outputs_blstm, (hn, cn) = self.blstm(outputs_packd)\n",
    "\n",
    "        # blstm训练完再把padding补上\n",
    "        outputs_paded, outputs_lengths = pad_packed_sequence(outputs_blstm)\n",
    "\n",
    "        # 调整形状，batch_size放在下标为0的维度\n",
    "        outputs_paded = outputs_paded.transpose(0, 1)#把batch放的第0维度\n",
    "\n",
    "        # 线性层\n",
    "        outputs_logits = self.linear(outputs_paded)#outputs_logits就是一句话的发射概率矩阵\n",
    "\n",
    "        # 取出每句话真实长度发射概率矩阵\n",
    "        # 最终输入到crf 作为emission_score\n",
    "        outputs = []\n",
    "\n",
    "        for outputs_logit, outputs_length in zip(outputs_logits, outputs_lengths):#为啥要循环，因为一个batch中有多个句子\n",
    "            outputs.append(outputs_logit[:outputs_length])#把这句话除了padding外的真实的部分加入outputs。\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        output_embed = self.embed(inputs)\n",
    "        # print('output_embed.shape:', output_embed.shape)\n",
    "\n",
    "        # 在batch size增加一个维度1\n",
    "        output_embed = output_embed.unsqueeze(1)\n",
    "        # print('output_embed.shape1:', output_embed.shape)\n",
    "\n",
    "        output_blstm, (hn, cn) = self.blstm(output_embed)\n",
    "\n",
    "        output_blstm = output_blstm.squeeze(1)\n",
    "\n",
    "        output_linear = self.linear(output_blstm)\n",
    "\n",
    "        return output_linear\n",
    "#这里可以直接用BiLSTM的输出结果作为答案，但是为什么不用呢\n",
    "#因为CRF可以学习一些先验知识，比如一句话开头不能是E（BMSE）"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c0c7903011596f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2构建CRF"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbead7dbbf94f4de"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CRF(nn.Module):\n",
    "    def __init__(self, label_num):\n",
    "        super(CRF, self).__init__()\n",
    "\n",
    "        # todo:定义标签数量\n",
    "        self.label_num  = label_num#状态的数量,B-Person, I-Person, B-Organization, I-Organization, O\n",
    "\n",
    "        # todo:随机初始化转移分数，模型需要学习的参数\n",
    "        self.transition_scores = nn.Parameter(torch.randn(self.label_num+2, self.label_num+2))\n",
    "        # +2原因，在B-Person, I-Person, B-Organization, I-Organization, O基础上增加了START_TAG END_TAG\n",
    "        #转移概率矩阵是CRF模型学习的重要参数，他是通过nn.Parameter定义的，后续会通过optimizer.step()自动更新\n",
    "\n",
    "\n",
    "        # 设置start_tag end_tag的编号\n",
    "        self.START_TAG, self.END_TAG = self.label_num, self.label_num+1\n",
    "\n",
    "        # todo:初始化start end--保证其他状态不会转到START 到END之后不会再转到其他状态\n",
    "        # 定义填充的常量\n",
    "        self.fill_value = -1000\n",
    "        self.transition_scores.data[:, self.START_TAG] = self.fill_value#这里设置成-1000怎么计算都永远不可能是最大值\n",
    "        self.transition_scores.data[self.END_TAG, :] = self.fill_value#\n",
    "\n",
    "\n",
    "    # todo:1-计算单条路径的分数Sreal\n",
    "    def _get_real_path_score(self, emission_score, sequence_label):\n",
    "        # emission_score是BilSTM输出结果\n",
    "        # sequence_label自己标注的真实的序列标签\n",
    "\n",
    "        # 前一时刻到后一时刻转换的时候的累加\n",
    "\n",
    "        # 保存sequence长度\n",
    "        seq_len = len(sequence_label)#序列长度，多少个字\n",
    "\n",
    "        # todo:1-计算发射分数\n",
    "        # emission_score中每一行是一个字的发射分数，就是发射成B-Person, I-Person, B-Organization, I-Organization, O各自的概率\n",
    "        real_emission_score = torch.sum(emission_score[list(range(seq_len)), sequence_label])\n",
    "        # wzy:每一时刻发射到对应标签上的概率值累加\n",
    "        # 这里是拿着真实标签sequence_label去训练得来的emission_score这个矩阵中找真实标签对应的分数\n",
    "\n",
    "        #todo:2-计算转移路径分数\n",
    "        #todo:2-1先把原序列加上 start 和 end\n",
    "        #[1, 2, 3] sequence_label\n",
    "        b_id = torch.tensor([self.START_TAG], dtype=torch.int32, device=device)\n",
    "        e_id = torch.tensor([self.END_TAG], dtype=torch.int32,  device=device)\n",
    "        sequence_label_expand = torch.cat([b_id, sequence_label, e_id])\n",
    "        # 再真实标签前后增加 start end\n",
    "        # 结果类似 [5, 1, 2, 3, 6]\n",
    "\n",
    "        # todo:2-2获取转移路径\n",
    "        pre_tag = sequence_label_expand[list(range(seq_len+1))]\n",
    "        # 结果类似 [5, 1, 2, 3]\n",
    "        now_tag = sequence_label_expand[list(range(1, seq_len+2))]\n",
    "        # 结果类似 [1, 2, 3, 6]\n",
    "        real_transition_score = torch.sum(self.transition_scores[pre_tag, now_tag])\n",
    "        # wzy:这样错开一位构造十分巧妙，表示从前一个状态转移到后一个状态的转移值\n",
    "        # 去转移概率矩阵中就可以拿到这个值了\n",
    "\n",
    "        # todo:3-真实路径分数 = 发射分数 + 转移路径分数\n",
    "        real_path_score = real_emission_score + real_transition_score\n",
    "\n",
    "        return real_path_score\n",
    "\n",
    "    #todo:2-2计算log_exp值\n",
    "    def _log_sum_exp(self, score):\n",
    "        # 这部分的公式就是pdf第7页最上面的那个\n",
    "        # 根据公式计算路径的分数\n",
    "        # 为了避免计算时溢出，每个元素先减去最大值，计算完成后，再把最大值加回来\n",
    "        max_score, _ = torch.max(score, dim=0)\n",
    "        max_score_expand = max_score.expand(score.shape)#每个元素都减去max值\n",
    "        return max_score + torch.log(torch.sum(torch.exp(score-max_score_expand)))\n",
    "\n",
    "    #todo:2-1扩展发射概率矩阵增加两行两列\n",
    "    # 两行指的是在一句话开头增加的start字符结尾增加的end字符\n",
    "    # 两列指的是开始的tag和结束的tag\n",
    "    def _expand_emission_matrix(self, emission_score):  #emission_score:[seq_len,tag_len][3,5]\n",
    "        # 对发射分数进行扩充，因为添加了start end两个标签\n",
    "        # emission_score的形状\n",
    "        # [字的个数, 5] 5代表的是 len [B-dis I-dis B-sym I-sym O]\n",
    "        # 获取序列长度\n",
    "        # 比如emission_score对应的是 我头疼 的发射分数矩阵\n",
    "        # 是 3 * 5矩阵\n",
    "        seq_length = emission_score.shape[0]\n",
    "\n",
    "        # todo:前期准备1--增加start end这两个标签\n",
    "        b_s = torch.tensor([[self.fill_value] * self.label_num + [0, self.fill_value]],device=device)\n",
    "        # b_s e_s 都是1 * 7向量,b_s这7个数分别是[-1000,-1000,-1000,-1000,-1000,0,-1000]\n",
    "        e_s = torch.tensor([[self.fill_value] * self.label_num + [self.fill_value, 0]],device=device)\n",
    "        #e_s : [-1000,-1000,-1000,-1000,-1000,-1000,0]\n",
    "\n",
    "        # todo:前期准备2--先初始化\n",
    "        expand_matrix = self.fill_value * torch.ones([seq_length, 2], dtype=torch.float32, device=device)\n",
    "        # 3 * 2 值全为-1000\n",
    "\n",
    "        # todo:1-[3,5]->[3,7]加两列\n",
    "        emission_score_expand = torch.cat([emission_score, expand_matrix], dim=1)# 3 * 7\n",
    "\n",
    "        # todo:2-[3,7]->[5,7]加两行\n",
    "        emission_score_expand = torch.cat([b_s, emission_score_expand, e_s], dim=0)# 5 * 7\n",
    "\n",
    "        return emission_score_expand\n",
    "\n",
    "    #todo:2-获取全部路径分数\n",
    "    def _get_total_path_score(self, emission_score):\n",
    "\n",
    "        # todo:1- 扩展发射分数矩阵\n",
    "        emission_score_expand = self._expand_emission_matrix(emission_score)\n",
    "\n",
    "        # 计算所有路径分数\n",
    "        pre = emission_score_expand[0] # pre代表的是累计到上一个时刻，每个状态之前的所有路径分数之和,相当于课件上的alpha\n",
    "        # pre的形状是[1*7]，初始时刻表示第一个字的7个发射分数\n",
    "        for obs in emission_score_expand[1:]:\n",
    "            # todo:1-扩展pre的维度，把pre转置，横向广播一个维度，pre就是课件中的alpha\n",
    "            pre_expand = pre.reshape(-1, 1).expand([self.label_num+2, self.label_num+2])\n",
    "            # pre本来是一行，reshape后变成一列[7,1]，expand后变成[7,7]7行7列，每一行内容相同，这个矩阵没有什么意义只是为了后续计算方便\n",
    "\n",
    "            # todo:2-扩展obs的维度，纵向添加一个维度\n",
    "            obs_expand = obs.expand([self.label_num+2, self.label_num+2])\n",
    "            #obs是一行,[1,7]这里扩展成[7,7]其实就是复制，这个矩阵也没有什么意义，只是为了计算方便\n",
    "\n",
    "\n",
    "            # todo:3-按照矩阵计算的目录，计算上一个时刻的每种状态 到这个时刻的每种状态的组合方式全部包含在矩阵运算\n",
    "            score = obs_expand + pre_expand + self.transition_scores#transition_scores不用变\n",
    "            #该时刻的发射分数 + 累积到该时刻前所有时刻的总分数 + 上一时刻转移到该时刻的转移分数矩阵\n",
    "\n",
    "            # todo:4-计算分数,就是对第三步的结果进行log_sum_exp\n",
    "            # print('\\nscore:', score)\n",
    "            # print('\\nscore.shape:', score.shape)\n",
    "            pre = self._log_sum_exp(score)\n",
    "            # 1 x 7 每一列代表的是上一个时刻的所有状态到这个时刻的某一个状态之和\n",
    "        # for结束仍然得到一个pre 代表是最后一个时刻, 1 x 7 每一列代表的是上一个时刻的所有状态到这个时刻的某一个状态之和\n",
    "\n",
    "        # 因为for循环执行完成后，pre最后一个时刻，也就是每个状态之前的所有路径之和\n",
    "        # 最终结果计算全部路径之和，因此还需要进行最后一步计算\n",
    "        return self._log_sum_exp(pre)\n",
    "\n",
    "    def forward(self, emission_scores, sequence_labels):\n",
    "        # todo:计算损失值\n",
    "        # 是一个批次的\n",
    "        total = 0.0\n",
    "        for emission_score, sequence_label in zip(emission_scores, sequence_labels):\n",
    "            real_path_score = self._get_real_path_score(emission_score, sequence_label)\n",
    "            total_path_score = self._get_total_path_score(emission_score)\n",
    "            loss = total_path_score - real_path_score\n",
    "            total += loss\n",
    "\n",
    "        return total\n",
    "\n",
    "    #todo:3-使用维特比算法进行预测\n",
    "    def predict(self, emission_score):\n",
    "        # todo:1-扩展emission_score-就是把start end加到矩阵中去\n",
    "        emission_score_expand = self._expand_emission_matrix(emission_score)\n",
    "\n",
    "        #todo:2- 记录每个时刻对应 每个状态对应的 最大分数，以及索引\n",
    "        ids = torch.zeros(1, self.label_num+2, dtype=torch.long, device=device)\n",
    "        val = torch.zeros(1, self.label_num+2, device=device)\n",
    "\n",
    "        pre = emission_score_expand[0]\n",
    "\n",
    "        for obs in emission_score_expand[1:]:\n",
    "            # 对pre进行旋转\n",
    "            pre_extend = pre.reshape(-1, 1).expand([self.label_num+2, self.label_num+2])\n",
    "            obs_extend = obs.expand([self.label_num+2, self.label_num+2])\n",
    "\n",
    "            # 累加，矩阵对用位置进行累加，得到的结果是上一个时刻的所有状态到这个时刻的所有状态可能转移方式\n",
    "            score = obs_extend + pre_extend + self.transition_scores\n",
    "\n",
    "            # todo:不同点 -- 记录当前时刻最大的分值和索引\n",
    "            value, index = score.max(dim=0)#取出每一个字的最大得分和对应的索引\n",
    "            ids = torch.cat([ids, index.unsqueeze(0)], dim=0)#保存的是上一步的index和这一步的index[seq_len,2]\n",
    "            val = torch.cat([val, value.unsqueeze(0)], dim=0)\n",
    "            #wzy：维特比算法和前向算法最大的区别就是维特比这里记录了最大值的下标，仅仅就是这一点差别\n",
    "\n",
    "            pre = value\n",
    "\n",
    "        # todo:取出最后一个时刻的最大值\n",
    "        index = torch.argmax(val[-1])\n",
    "        best_path = [index]\n",
    "        # print('val[-1]:', val[-1])\n",
    "        # print('best_path:', best_path)\n",
    "\n",
    "        for i in reversed(ids[1:]):#reversed反转从后往前找\n",
    "            index = i[index].item()#根据上一步的index找到这一步的index\n",
    "            best_path.append(index)\n",
    "            # print(i, 'best_path:', best_path)\n",
    "\n",
    "        best_path = best_path[::-1][1:-1]#再给他反转回来\n",
    "\n",
    "        return best_path"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48eabdbb15aef2cf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.数据处理"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adcf6150a165d0c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1处理词表"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47ef57f30ec3b5f8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def build_vocab():\n",
    "\n",
    "    chat_to_id = json.load(open('ner_data/char_to_id.json', mode='r', encoding='utf-8'))\n",
    "    unique_words = list(chat_to_id.keys())[1:-1]#去掉一头一尾\n",
    "    unique_words.insert(0, '[UNK]')\n",
    "    unique_words.insert(0, '[PAD]')\n",
    "\n",
    "    # 把数据写入到文本中\n",
    "    with open('ner_data/bilstm_crf_vocab_aidoc.txt', 'w') as file:\n",
    "        for word in unique_words:\n",
    "            file.write(word+'\\n')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecb0dace4cfb8bbe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2把单个字用空格连接"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b01a1dfd04a6ad1d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "train_data_file_path = './ner_data/train.txt'\n",
    "valid_data_file_path = './ner_data/validate.txt'\n",
    "\n",
    "def process_text_file(input_path, output_csv_name):\n",
    "    data_inputs = []\n",
    "    data_labels = []\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            # 把每个样本的字和标签用空格连接\n",
    "            data_inputs.append(' '.join(data['text']))\n",
    "            data_labels.append(' '.join(data['label']))\n",
    "\n",
    "    # 转换为 DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'data_inputs': data_inputs,\n",
    "        'data_labels': data_labels\n",
    "    })\n",
    "\n",
    "    # 保存为 CSV 文件\n",
    "    output_path = f'ner_data/{output_csv_name}.csv'\n",
    "    df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    print(f\"{output_csv_name} 数据量:\", len(df))\n",
    "\n",
    "\n",
    "\n",
    "process_text_file(train_data_file_path, 'train')\n",
    "process_text_file(valid_data_file_path, 'valid')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ef85aff55d542224",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'f_code'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 122\u001B[0m\n\u001B[0;32m    119\u001B[0m             model\u001B[38;5;241m.\u001B[39msave_model(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel/BiLSTM-CRF-\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.bin\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    121\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m--> 122\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[1], line 97\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     94\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m     95\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m---> 97\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[0;32m     98\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m    100\u001B[0m     \u001B[38;5;66;03m# 前向传播\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[1;32mIn[1], line 39\u001B[0m, in \u001B[0;36mNERDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;66;03m# 标签转换\u001B[39;00m\n\u001B[0;32m     36\u001B[0m label_ids \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_map\u001B[38;5;241m.\u001B[39mget(label, \u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m label \u001B[38;5;129;01min\u001B[39;00m labels][:\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_length]\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[1;32m---> 39\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39mtensor(input_ids, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong),\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m: torch\u001B[38;5;241m.\u001B[39mtensor(label_ids, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong),\n\u001B[0;32m     41\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlength\u001B[39m\u001B[38;5;124m'\u001B[39m: torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;28mlen\u001B[39m(input_ids), dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)\n\u001B[0;32m     42\u001B[0m }\n",
      "Cell \u001B[1;32mIn[1], line 39\u001B[0m, in \u001B[0;36mNERDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;66;03m# 标签转换\u001B[39;00m\n\u001B[0;32m     36\u001B[0m label_ids \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_map\u001B[38;5;241m.\u001B[39mget(label, \u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m label \u001B[38;5;129;01min\u001B[39;00m labels][:\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_length]\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[1;32m---> 39\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39mtensor(input_ids, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong),\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m: torch\u001B[38;5;241m.\u001B[39mtensor(label_ids, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong),\n\u001B[0;32m     41\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlength\u001B[39m\u001B[38;5;124m'\u001B[39m: torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;28mlen\u001B[39m(input_ids), dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)\n\u001B[0;32m     42\u001B[0m }\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:764\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mD:\\new_software\\PyCharm\\plugins\\python\\helpers-pro\\jupyter_debug\\pydev_jupyter_plugin.py:92\u001B[0m, in \u001B[0;36mcan_not_skip\u001B[1;34m(plugin, pydb, frame, info)\u001B[0m\n\u001B[0;32m     90\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcan_not_skip\u001B[39m(plugin, pydb, frame, info):\n\u001B[0;32m     91\u001B[0m     step_cmd \u001B[38;5;241m=\u001B[39m info\u001B[38;5;241m.\u001B[39mpydev_step_cmd\n\u001B[1;32m---> 92\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m step_cmd \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m108\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[43m_is_equals\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_get_stop_frame\u001B[49m\u001B[43m(\u001B[49m\u001B[43minfo\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m     93\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m pydb\u001B[38;5;241m.\u001B[39mjupyter_breakpoints:\n",
      "File \u001B[1;32mD:\\new_software\\PyCharm\\plugins\\python\\helpers-pro\\jupyter_debug\\pydev_jupyter_plugin.py:126\u001B[0m, in \u001B[0;36m_is_equals\u001B[1;34m(frame, other_frame)\u001B[0m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_is_equals\u001B[39m(frame, other_frame):\n\u001B[0;32m    123\u001B[0m     \u001B[38;5;66;03m# We can't compare frames directly, because Jupyter compiles ast nodes\u001B[39;00m\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;66;03m# in cell separately. At the same time, the frame filename is unique and stays\u001B[39;00m\n\u001B[0;32m    125\u001B[0m     \u001B[38;5;66;03m# the same within a cell.\u001B[39;00m\n\u001B[1;32m--> 126\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m frame\u001B[38;5;241m.\u001B[39mf_code\u001B[38;5;241m.\u001B[39mco_filename \u001B[38;5;241m==\u001B[39m \u001B[43mother_frame\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf_code\u001B[49m\u001B[38;5;241m.\u001B[39mco_filename \\\n\u001B[0;32m    127\u001B[0m            \u001B[38;5;129;01mand\u001B[39;00m ((frame\u001B[38;5;241m.\u001B[39mf_code\u001B[38;5;241m.\u001B[39mco_name\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m<cell line:\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    128\u001B[0m                  \u001B[38;5;129;01mand\u001B[39;00m other_frame\u001B[38;5;241m.\u001B[39mf_code\u001B[38;5;241m.\u001B[39mco_name\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m<cell line:\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m    129\u001B[0m                 \u001B[38;5;129;01mor\u001B[39;00m frame\u001B[38;5;241m.\u001B[39mf_code\u001B[38;5;241m.\u001B[39mco_name \u001B[38;5;241m==\u001B[39m other_frame\u001B[38;5;241m.\u001B[39mf_code\u001B[38;5;241m.\u001B[39mco_name)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'f_code'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from bilstm_crf import NER\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 自定义数据集类\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, label_map, max_length=128):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_map = label_map\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['data_inputs']\n",
    "        labels = self.data.iloc[idx]['data_labels'].split()\n",
    "        \n",
    "        # 文本编码\n",
    "        input_ids = self.tokenizer.encode(\n",
    "            text, \n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        # 标签转换\n",
    "        label_ids = [self.label_map.get(label, 0) for label in labels][:self.max_length]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(label_ids, dtype=torch.long),\n",
    "            'length': torch.tensor(len(input_ids), dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 数据整理函数\n",
    "def collate_fn(batch):\n",
    "    # 按长度降序排序\n",
    "    batch.sort(key=lambda x: x['length'], descending=True)\n",
    "    \n",
    "    # 解包数据\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    lengths = torch.stack([item['length'] for item in batch])\n",
    "    \n",
    "    # 填充序列\n",
    "    padded_inputs = pad_sequence(input_ids, batch_first=True)\n",
    "    padded_labels = pad_sequence(labels, batch_first=True)\n",
    "    \n",
    "    return {\n",
    "        'inputs': padded_inputs.to(device),\n",
    "        'labels': padded_labels.to(device),\n",
    "        'lengths': lengths.to(device)\n",
    "    }\n",
    "\n",
    "# 训练函数\n",
    "def train():\n",
    "    # 标签映射\n",
    "    label_to_index = {\"O\":0, \"B-dis\":1, \"I-dis\":2, \"B-sym\":3, \"I-sym\":4}\n",
    "    \n",
    "    # 初始化分词器\n",
    "    tokenizer = BertTokenizer(vocab_file='ner_data/bilstm_crf_vocab_aidoc.txt')\n",
    "    \n",
    "    # 创建数据集和数据加载器\n",
    "    train_dataset = NERDataset('ner_data/train.csv', tokenizer, label_to_index)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        # num_workers=2\n",
    "    )\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = NER(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        label_num=len(label_to_index)\n",
    "    ).to(device)\n",
    "    \n",
    "    # 优化器\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-5)\n",
    "    \n",
    "    # 训练循环\n",
    "    num_epochs = 300\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向传播\n",
    "            loss = model(\n",
    "                batch['inputs'],\n",
    "                batch['labels'],\n",
    "                batch['lengths']\n",
    "            )\n",
    "            \n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # 打印统计信息\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # 保存模型\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            model.save_model(f'model/BiLSTM-CRF-{epoch+1}.bin')\n",
    "\n",
    "print(1)\n",
    "train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-22T14:51:37.428059Z",
     "start_time": "2025-04-22T14:51:00.861561Z"
    }
   },
   "id": "dc0a1c61e7841de5",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3632b7064ba9f1c1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
